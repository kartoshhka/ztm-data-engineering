{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e23f923",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Housing price prediction pipeline\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e8bec1-9c23-45cc-b4d8-de2e469ac42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- sqft_living: integer (nullable = true)\n",
      " |-- sqft_lot: integer (nullable = true)\n",
      " |-- floors: double (nullable = true)\n",
      " |-- waterfront: integer (nullable = true)\n",
      " |-- view: integer (nullable = true)\n",
      " |-- condition: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- sqft_above: integer (nullable = true)\n",
      " |-- sqft_basement: integer (nullable = true)\n",
      " |-- yr_built: integer (nullable = true)\n",
      " |-- yr_renovated: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- sqft_living15: integer (nullable = true)\n",
      " |-- sqft_lot15: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"data/kc_house_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "514963ac-4f57-4595-a01d-13952640affe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+\n",
      "|price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|grade|sqft_above|sqft_basement|yr_built|\n",
      "+-----+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+\n",
      "|    0|       0|        0|          0|       0|     0|         0|   0|        0|    0|         0|            0|       0|\n",
      "+-----+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+\n",
      "\n",
      "+-----+--------+---------+-----------+--------+------+----------+-----+---------+-----+----------+-------------+--------+\n",
      "|price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront| view|condition|grade|sqft_above|sqft_basement|yr_built|\n",
      "+-----+--------+---------+-----------+--------+------+----------+-----+---------+-----+----------+-------------+--------+\n",
      "|    0|      13|       10|          0|       0|     0|     21450|19489|        0|    0|         0|        13126|       0|\n",
      "+-----+--------+---------+-----------+--------+------+----------+-----+---------+-----+----------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "filtered_data = data.drop('id', 'date', 'sqft_living15', 'sqft_lot15', 'zipcode', 'lat','long', 'yr_renovated')\n",
    "\n",
    "filtered_data \\\n",
    "    .select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in filtered_data.columns]) \\\n",
    "    .show()\n",
    "\n",
    "# Count how many rows will be dropped\n",
    "filtered_data.select([\n",
    "    F.count(F.when(F.col(c) == 0, 1)).alias(c) for c in filtered_data.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f38e3ea-9f91-40b3-9a21-3c35cf0a24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = filtered_data.filter((F.col(\"bedrooms\") != 0) & (F.col(\"bathrooms\") != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4168050c-2b9b-4f7e-aa09-cfc4dab77c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  17288\n",
      "Test size:  4309\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data_cleaned.randomSplit([0.8, 0.2], seed=24)\n",
    "print(\"Train size: \", train_data.count())\n",
    "print(\"Test size: \", test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe1dc64b-1b0c-43b6-85fc-7424e56c6d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|grade|\n",
      "+-----+\n",
      "|   12|\n",
      "|   13|\n",
      "|    6|\n",
      "|    5|\n",
      "|    9|\n",
      "|    4|\n",
      "|    8|\n",
      "|    7|\n",
      "|   10|\n",
      "|   11|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.select('grade') \\\n",
    "  .distinct() \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f49e1567-6e0b-424d-92c5-7c40617984af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "encoders= OneHotEncoder(inputCols=['waterfront', 'view', 'condition', 'grade'], \\\n",
    "                           outputCols=['waterfront_vec', 'view_vec', 'condition_vec', 'grade_vec'], \\\n",
    "                        dropLast=False)\n",
    "\n",
    "feature_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "                'waterfront_vec', 'view_vec', 'condition_vec', 'grade_vec',\n",
    "                'sqft_above', 'sqft_basement', 'yr_built']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price')\n",
    "\n",
    "pipeline = Pipeline(stages=[encoders, assembler, lr])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2739a543-62f6-422a-bb76-5b94f131e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2f8f441b-e8a0-4622-869f-0822493ff2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 132483.95305184484\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='mae')\n",
    "mae = evaluator_mae.evaluate(test_predictions)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0692d-8101-4c03-a22b-31482c2b1d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
